# -*- coding: utf-8 -*-
"""Updated Datathon.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/195zl0ZSmO2oSvsnfyu-G7cQVVog3yWta

# Pip Installs
"""

!pip install bertopic
# Choose an embedding backend
!pip install bertopic[gensim]

# Topic modeling with images
!pip install bertopic[vision]

pip install spacy

pip install transformers

pip install google-generativeai pandas

pip install reportlab

pip install fpdf

"""# Imports"""

from bertopic import BERTopic
from google.colab import drive
drive.mount('/content/drive/')

# import libraries for data manipulation
import numpy as np
import pandas as pd

# import libraries for data visualization
import matplotlib.pyplot as plt
import seaborn as sns

# import libraries for textual cleaning
import spacy

from PIL import Image
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator

import google.generativeai as genai

from reportlab.lib.pagesizes import letter
from reportlab.pdfgen import canvas
from fpdf import FPDF

"""# EDA(News Articles)

"""

# read the data
df2 = pd.read_excel('/content/drive/MyDrive/Datathon 2025/Dataset/news_excerpts_parsed.xlsx')
# returns the first 6 rows
df2.head(6)

news = df2.copy()

# Create a wordcloud for first analysis of uncleaned and unprepped textual data
# Join all the words into one string
text = " ".join(Text for Text in news.Text)

# Create stopword list:
stopwords = set(STOPWORDS)
stopwords.update([])

# Generate a word cloud image
wordcloud = WordCloud(stopwords=stopwords, background_color="white").generate(text)

# Display the generated image:
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()

"""It is clear from the wordcloud that the news articles revolve around a wide range of topics. However, it is clear that a large part of the article talks about Singapore and China. On the other hand, logically so, news articles take in excrepts from interviews and thus "**said**" becomes a very prominent word."""

# Load spaCy's English model for textual data cleaning
nlp = spacy.load("en_core_web_sm")
stop_words = nlp.Defaults.stop_words

# Tokenize the cleaned data
for i in range(len(news["Text"])):
  doc = nlp(news["Text"][i])
  filtered_text = [token.text for token in doc if not token.is_stop]
  news["Text"][i] = filtered_text

#Combine the cleaned tokens into a coherent string
for i in range(len(news["Text"])):
  text = ' '
  for j in range(len(news["Text"][i])):
    text += news["Text"][i][j] + ' '
  news["Text"][i] = text

news.head(6)

"""The dataset is now cleaned through spacy. Thus, allowing us to utilise this for a model later on."""

text = " ".join(Text for Text in news.Text)

# Create stopword list:
stopwords = set(STOPWORDS)
stopwords.update([])

# Generate a word cloud image
wordcloud = WordCloud(stopwords=stopwords, background_color="white").generate(text)

# Display the generated image:
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()

"""The cleaned word cloud presents a relationship that Singapore and China took on an even larger significance. It is clear that there is also a large chunk of news that are related to corporations.

# News Articles

Now that we know the possible relationships, we can now train a model on the Textual data using BERTopic. While we used Topic Wizard for the dashboard, BERTopic allowed us a greater range of outputs that we could play with such as the topics assigned to each document and the representative words for each topic in a dataframe format. Thus, why we chose BERTopic.
"""

topic_model = BERTopic()
topics, probs = topic_model.fit_transform(news["Text"])

topic_distr, _ = topic_model.approximate_distribution(news["Text"])

topic_model.visualize_distribution(topic_distr[1])

"""This is the start of our solution. Where users are able to key in queries into the model and the model will then create a new vector for that query. After which, the model finds similar topics around that model. In this case the query "Threats to Singapore", returns the word 'Covid19'. Allowing us to create an output dataframe that contains only articles related to Covid19."""

similar_topics, similarity = topic_model.find_topics("Threats to Singapore", top_n=10)
topic_model.get_topic(similar_topics[0])[0]
key_word = topic_model.get_topic(similar_topics[0])[0][0]

topics = topic_model.get_topics()
for topic, words in topics.items():
  for i in range(len(words)):
    if words[i][0] == key_word :
      key_topic = topic

topics = topic_model.topics_
df2["Topic"] = topics

# Filter rows where Topic matches key_topic and keep both Link & Text columns
output = df2[df2["Topic"] == key_topic][["Link", "Text"]].reset_index(drop=True)

# Display the first 6 rows
output.head(6)

"""We then use Gemini's API to generate a summary of the textual evidence based on the above manipulation."""

# Set up Gemini API
API_KEY = "YOUR_API_KEY"  
genai.configure(api_key=API_KEY)

# Combine all news articles into one text block
all_texts = "\n\n".join(f"Source: {row['Link']}\n{row['Text']}" for _, row in output.iterrows())

# Function to generate actionable insights
def generate_actionable_insights(texts):
    model = genai.GenerativeModel("gemini-pro")

    prompt = f"""
    Based on the following news articles, extract key **actionable insights**.
    The insights should be clear, practical, and impactful. Reference the sources where necessary.

    Articles:
    {texts}

    Provide a structured summary with bullet points highlighting key actions or takeaways that someone from the Ministry of Home Affairs and the Internal Security Deparment of Singapore could use. Only related to Singapore.
    Before any actionable insights, please summarise the threats that this poses onto Singaporean internal security.

    Start the text with first a title namely: "Threats to Singapore". Please remember to include the links that they took these insights from.

    Summary:
    """

    try:
        response = model.generate_content(prompt)
        return response.text.strip() if response else "Error: No response"

    except Exception as e:
        return f"Error: {e}"

# Get actionable insights
actionable_summary = generate_actionable_insights(all_texts)

# Display the result
print(actionable_summary)

"""The code block below allows us to save these insights into a PDF file."""

from fpdf import FPDF

def save_summary_as_pdf(summary_text, filename="Actionable_Insights.pdf"):
    pdf = FPDF()
    pdf.set_auto_page_break(auto=True, margin=15)  # Prevents text from running off the page
    pdf.add_page()

    # Title
    pdf.set_font("Arial", style="B", size=16)
    pdf.cell(200, 10, "Key Actionable Insights", ln=True, align='C')
    pdf.ln(10)  # Add spacing

    # Body text (Auto-wrapping)
    pdf.set_font("Arial", size=12)
    pdf.multi_cell(0, 10, summary_text)  # âœ… Fix: Automatically wraps text to fit page width

    # Save the PDF
    pdf.output(filename)
    print(f"PDF saved as {filename}")

# Save the summary as a PDF
save_summary_as_pdf(actionable_summary)

"""The dataframe below presents the other identified topics by the model. Allowing us to see the general trends and topics available. This would be used for the public version of this solution instead of the query method."""

topic_model.get_topic_info()

"""# Wikileaks"""

# read the data
df = pd.read_excel('/content/drive/MyDrive/Datathon 2025/Dataset/wikileaks_parsed.xlsx')
# returns the first 5 rows
df.head(6)

wikileaks = df.copy()

# First we need to also figure out how to contain each PDF onto one row. This allows us to train the model more efficiently
wikileaks[["PDF Number", "PDF"]] = wikileaks["PDF Path"].str.split(".pdf", expand = True)
for i in range(1, len(wikileaks)):
  if wikileaks["PDF Number"][i] == wikileaks["PDF Number"][i-1]:
    wikileaks["Text"][i] = wikileaks["Text"][i-1] + " \n\n " + wikileaks["Text"][i]
    wikileaks = wikileaks.drop([i-1])

#This allows us to reference the rows in a more coherent fashion
wikileaks = wikileaks.reset_index(drop=True)

wikileaks.head(6)

text = " ".join(Text for Text in wikileaks.Text)

# Create stopword list:
stopwords = set(STOPWORDS)
stopwords.update([])

# Generate a word cloud image
wordcloud = WordCloud(stopwords=stopwords, background_color="white").generate(text)

# Display the generated image:
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()

"""It is abundantly clear that the Wikileaks have something to do with the staff members at the Pristina Airpot. However, our AI model would revela another topic as shown below"""

# Load spaCy's English model for textual data cleaning
nlp = spacy.load("en_core_web_sm")
stop_words = nlp.Defaults.stop_words

# Tokenize the cleaned data
for i in range(len(wikileaks["Text"])):
  doc2 = nlp(wikileaks["Text"][i])
  filtered_text = [token.text for token in doc2 if not token.is_stop]
  wikileaks["Text"][i] = filtered_text

#Combine the cleaned tokens into a coherent string
for i in range(len(wikileaks["Text"])):
  text = ' '
  for j in range(len(wikileaks["Text"][i])):
    text += wikileaks["Text"][i][j] + ' '
  wikileaks["Text"][i] = text

wikileaks.head(6)

topic_model2 = BERTopic()
topics2, probs2 = topic_model2.fit_transform(wikileaks["Text"])

topic_distr2, _ = topic_model2.approximate_distribution(wikileaks["Text"])
topic_model2.visualize_distribution(topic_distr[1])

topic_model2.get_topic_info()

"""It is clear that the model is valid given that a read through of the PDF files presents two large topics naemly: the Pristina Airport Issue and the US-Japanese Washington Summit."""
